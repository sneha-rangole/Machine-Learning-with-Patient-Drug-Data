{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":3507850,"datasetId":1972526,"databundleVersionId":3560437}],"dockerImageVersionId":31286,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Complete Exploratory Data Analysis and Predictive Modeling on Restaurant Cuisine Ratings\n\n- **Dataset:** Cuisine_rating.csv  \n- **Domain:** Restaurant Analytics / Consumer Behavior  \n- **Type:** Supervised Machine Learning (Regression)\n---","metadata":{}},{"cell_type":"markdown","source":"## 1. Introduction\n\n### 1.1 What is Exploratory Data Analysis (EDA)?\n\nExploratory Data Analysis, commonly referred to as EDA, is the process of analyzing and investigating a dataset before applying any statistical models or machine learning algorithms. It is the foundation of any data science project. EDA helps us understand the structure, patterns, distributions, anomalies, and relationships hidden within data.\n\nThink of EDA as a detective's initial investigation at a crime scene. Before drawing conclusions, a detective first looks at all the available evidence, tries to understand what happened, identifies inconsistencies, and only then forms a theory. In data science, EDA is that investigative phase.\n\nEDA involves:\n- Summarizing the main characteristics of a dataset using descriptive statistics.\n- Visualizing data using plots such as histograms, bar charts, boxplots, and heatmaps.\n- Identifying missing values, duplicates, and inconsistencies.\n- Discovering correlations and relationships between variables.","metadata":{}},{"cell_type":"markdown","source":"### 1.2 Why is EDA Important in Real-World Data Science?\n\nIn the real world, data is almost never clean or ready to use directly. Data arrives from multiple sources, is collected by different people, and contains errors, missing values, and inconsistencies. If you skip EDA and directly apply a machine learning model, you risk:\n\n- Training a model on incorrect or biased data, leading to completely wrong predictions.\n- Misunderstanding the problem, resulting in solving the wrong question.\n- Missing critical patterns that could lead to valuable business insights.\n- Building a model that performs well on paper but fails entirely in production.\n\nEDA saves you from all of these risks. According to industry surveys, professional data scientists spend approximately 60-80% of their project time on data preparation and EDA, not on model building. This reflects how critical EDA is.","metadata":{}},{"cell_type":"markdown","source":"### 1.3 What is Predictive Modeling?\n\nPredictive modeling is the process of using statistical algorithms and machine learning techniques to forecast future outcomes based on historical data. After we complete EDA and understand our dataset, we use the cleaned and prepared data to train machine learning models that can predict outcomes for new, unseen data.\n\nFor example: Given information about a restaurant customer (their age, gender, preferred cuisine, alcohol habits), can we predict the rating they would give to a restaurant? This is the kind of question predictive modeling answers.\n\nPredictive modeling can be of two types:\n- **Regression:** Predicting a continuous numerical value (e.g., predicting a rating score like 3.5 or 4.2).\n- **Classification:** Predicting a category or class (e.g., predicting whether a customer will return or not).\n\nIn this project, since our target variable (Overall Rating) is a continuous numerical value, we will perform regression.","metadata":{}},{"cell_type":"markdown","source":"### 1.4 Objective of This Project\n\nThe objective of this project is to:\n1. Thoroughly explore the restaurant cuisine ratings dataset to understand customer demographics, preferences, and behavior.\n2. Clean and prepare the data for machine learning.\n3. Engineer meaningful features that improve model performance.\n4. Build and evaluate multiple regression models to predict the overall rating a customer would give.\n5. Extract actionable business insights that restaurant owners and managers can use to improve their services.","metadata":{}},{"cell_type":"markdown","source":"### 1.5 What Insights Do We Want to Extract?\n\nWe want to answer the following business questions:\n- Which cuisine type receives the highest average ratings?\n- Does the gender of a customer influence how they rate a restaurant?\n- Does the customer's marital status or profession affect their rating behavior?\n- Does alcohol consumption frequency correlate with higher or lower ratings?\n- Which demographic segment is the most generous with ratings?\n- What are the most important factors that drive restaurant ratings?","metadata":{}},{"cell_type":"markdown","source":"### 1.6 Business Value of This Dataset\n\nFor restaurant owners, managers, and investors, understanding customer rating behavior is invaluable. A one-star difference in a restaurant's average rating can translate to a significant change in revenue. Research from Harvard Business School found that a one-star increase in Yelp rating leads to a 5-9% increase in revenue. Therefore:\n\n- Knowing which demographic gives low ratings helps identify service gaps.\n- Understanding cuisine preferences by region helps in menu planning.\n- Knowing the relationship between budget and satisfaction helps in pricing strategy.\n- Predicting ratings before they are submitted allows for proactive service improvement.\n\n---","metadata":{}},{"cell_type":"markdown","source":"## 2. Importing Libraries\n\n### Why Do We Import Libraries?\n\nPython by itself is a general-purpose programming language. It does not natively support data manipulation, statistical computations, or machine learning. Libraries are pre-written collections of code that extend Python's capabilities. By importing these libraries, we avoid reinventing the wheel and gain access to thousands of optimized functions built by experts.\n\nWithout importing these libraries, you would need to write thousands of lines of code from scratch just to load a CSV file or draw a simple chart. Libraries save enormous amounts of time and reduce the chance of error.\n\n### Library Explanations:\n\n**pandas:** Short for \"Panel Data.\" It is the most important library for data manipulation in Python. It provides the DataFrame object, which is like an Excel spreadsheet within Python. It allows you to load, clean, filter, transform, and summarize data with very concise code.\n\n**numpy:** Short for \"Numerical Python.\" It provides support for large multi-dimensional arrays and matrices, along with a large collection of mathematical functions. NumPy is the backbone that pandas and scikit-learn are built upon.\n\n**Difference between numpy and pandas:** NumPy operates on arrays (think of a grid of numbers). Pandas is built on top of NumPy and adds labeled axes (rows and columns with names), making it much easier to work with real-world tabular data that has column headers and row indices. For purely numerical operations on arrays, NumPy is faster. For working with structured datasets with mixed data types, pandas is preferred.\n\n**matplotlib:** The foundational plotting library in Python. It gives you complete control over every aspect of a plot, from axes labels to tick marks. It is powerful but requires more code for complex visualizations.\n\n**seaborn:** Built on top of matplotlib, seaborn provides a higher-level interface for creating statistically informative and visually attractive plots with much less code. It integrates tightly with pandas DataFrames.\n\n**scikit-learn (sklearn):** The most widely used machine learning library in Python. It provides tools for preprocessing data, splitting datasets, building models, and evaluating them.\n\n**What happens if we do NOT import these libraries?** Python will raise a `NameError` or `ModuleNotFoundError` the moment you try to use any of these functions. Your code will not run at all.","metadata":{}},{"cell_type":"code","source":"# Importing All Required Libraries\n\n# Core data manipulation libraries\nimport pandas as pd          # For DataFrame operations and data manipulation\nimport numpy as np           # For numerical computations and array operations\n\n# Visualization libraries\nimport matplotlib.pyplot as plt   # Base plotting library\nimport seaborn as sns             # Statistical visualization on top of matplotlib\n\n# Machine learning - preprocessing\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n# LabelEncoder: converts text categories to integers (e.g., Male=0, Female=1)\n# StandardScaler: scales numerical features to have mean=0 and std=1\n\n# Machine learning - model selection\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n# train_test_split: splits data into training and testing sets\n# cross_val_score: evaluates model using k-fold cross validation\n# GridSearchCV: finds the best hyperparameters by trying all combinations\n\n# Machine learning - regression models\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Machine learning - evaluation metrics\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n# Pipeline for production-ready code\nfrom sklearn.pipeline import Pipeline\n\n# Suppress warnings for cleaner output\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configure matplotlib to display plots inline in the notebook\n%matplotlib inline\n\n# Set a consistent visual style for all seaborn plots\n# 'whitegrid' adds horizontal grid lines which help read values off charts\nsns.set_style('whitegrid')\n\n# Set the default figure size for all plots\nplt.rcParams['figure.figsize'] = (10, 6)\n\n# Set a random seed for reproducibility\n# This ensures that whenever you run this notebook, you get the same random results\nRANDOM_STATE = 42\n\nprint(\"All libraries imported successfully.\")\nprint(f\"Pandas version: {pd.__version__}\")\nprint(f\"NumPy version: {np.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-03-01T19:56:25.412872Z","iopub.execute_input":"2026-03-01T19:56:25.413168Z","iopub.status.idle":"2026-03-01T19:56:27.191434Z","shell.execute_reply.started":"2026-03-01T19:56:25.413142Z","shell.execute_reply":"2026-03-01T19:56:27.190709Z"}},"outputs":[{"name":"stdout","text":"All libraries imported successfully.\nPandas version: 2.3.3\nNumPy version: 2.0.2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"**What the output tells us:** Seeing the version numbers confirms that the libraries are installed correctly. Different versions of libraries can behave differently, so documenting versions is important for reproducibility in professional projects.\n\n---","metadata":{}},{"cell_type":"markdown","source":"## 3. Loading the Dataset\n\n### Why Do We Load Data This Way?\n\nWe use `pd.read_csv()` to load comma-separated value files directly into a pandas DataFrame. A DataFrame is a two-dimensional labeled data structure â€” think of it as an Excel table inside Python. It has rows and columns, each with labels.\n\n### What Happens if the Dataset Path is Wrong?\n\nPython will raise a `FileNotFoundError`. This is one of the most common beginner mistakes. Always ensure:\n1. The file name is spelled correctly (case-sensitive on Linux/Mac).\n2. The file is in the same directory as the notebook, OR you provide the full absolute path.\n3. The file extension is correct (.csv, not .CSV or .xlsx).\n\n### Common File Loading Errors:\n- `FileNotFoundError`: File does not exist at the specified path.\n- `UnicodeDecodeError`: The file uses a different character encoding (try `encoding='latin-1'` or `encoding='utf-8-sig'`).\n- `ParserError`: The file is malformed (e.g., inconsistent number of columns in some rows).","metadata":{}}]}